{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0320cc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install synonyms\n",
    "!pip install jieba\n",
    "!pip install jionlp\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e5f5aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from /Users/fengran/opt/anaconda3/lib/python3.8/site-packages/synonyms/data/vocab.txt ...\n",
      "Loading model from cache /var/folders/4q/zs_820g52y10wypzt_h_hhl40000gn/T/jieba.uf37e8e060469f7b6eac78422145738c0.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Synonyms: v3.18.0, Project home: https://github.com/chatopera/Synonyms/\n",
      "\n",
      " Project Sponsored by Chatopera\n",
      "\n",
      "  deliver your chatbots with Chatopera Cloud Services --> https://bot.chatopera.com\n",
      "\n",
      ">> Synonyms load wordseg dict [/Users/fengran/opt/anaconda3/lib/python3.8/site-packages/synonyms/data/vocab.txt] ... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 1.302 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Synonyms on loading stopwords [/Users/fengran/opt/anaconda3/lib/python3.8/site-packages/synonyms/data/stopwords.txt] ...\n",
      ">> Synonyms on loading vectors [/Users/fengran/opt/anaconda3/lib/python3.8/site-packages/synonyms/data/words.vector.gz] ...\n",
      "# jionlp - 微信公众号: JioNLP  Github: `https://github.com/dongrixinyu/JioNLP`.\n",
      "# jiojio - `http://www.jionlp.com/jionlp_online/cws_pos` is available for online use.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from augment import eda\n",
    "import random\n",
    "import jieba\n",
    "from jionlp import homophone_substitution, swap_char_position\n",
    "import wegt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.dirname(os.path.abspath(__file__))  # 获取当前目录位置\n",
    "data_path = dir_path+\"/CAIL2019-SCM\"\n",
    "if os.path.exists(data_path):\n",
    "    print(\"Data already exists!\")\n",
    "else:\n",
    "    os.makedirs(data_path)\n",
    "    url = \"https://cail.oss-cn-qingdao.aliyuncs.com/cail2019/CAIL2019-SCM.zip\"\n",
    "    wget.download(url, out=data_path)\n",
    "    print(\"Download!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "832647d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augment(sentence, seed=random.randint(1,10), alpha=0.1, nums=9):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentence:用于语义增强的句子\n",
    "        alpha:每条语句中将会被改变的单词数占比\n",
    "        seed:随机种子，是一个1-10之间的整数，当其大于7的时候，句子将不被增强\n",
    "        nums:每条原始语句增强的语句数\n",
    "    \"\"\"\n",
    "    if seed > 7:\n",
    "        #print(seed)\n",
    "        return sentence\n",
    "    ## 使用eda进行同义词替换的预处理\n",
    "    seg_list = jieba.cut(sentence)  # 使用jieba切词\n",
    "    seg_list = \" \".join(seg_list)  \n",
    "    words = list(seg_list.split())  # 获得单词\n",
    "    num_words = len(words)  # 获得单词数量\n",
    "    \n",
    "    nums_per_aug = nums//3  # 不同增强的具体数量\n",
    "\n",
    "    augmented_sentences = []\n",
    "    \n",
    "    ## 生成候选句子序列\n",
    "    ## 同义词替换\n",
    "    n_sr = max(1, int(alpha * num_words))  \n",
    "    for _ in range(nums_per_aug+1): \n",
    "        a_words = eda.synonym_replacement(words, n_sr)\n",
    "        augmented_sentences.append(''.join(a_words))\n",
    "    \n",
    "    ## 随机近义字替换\n",
    "    res1 = homophone_substitution(sentence,augmentation_num=nums_per_aug)\n",
    "    augmented_sentences.extend(res1)\n",
    "    \n",
    "    ## 邻近汉字换位\n",
    "    res2 = swap_char_position(sentence,augmentation_num=nums_per_aug)\n",
    "    augmented_sentences.extend(res2)\n",
    "    \n",
    "    ## 随机打乱句子顺序\n",
    "    random.shuffle(augmented_sentences)\n",
    "    #print(augmented_sentences)\n",
    "    \n",
    "    for aug_sentence in augmented_sentences:\n",
    "        if len(aug_sentence) == len(sentence): # 长度相等则直接输出\n",
    "            if aug_sentence == sentence:\n",
    "                continue\n",
    "            else:\n",
    "                return aug_sentence\n",
    "    else:\n",
    "        return sentence  # 否则直接输出句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fdd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2e512cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CAIL2019-SCM/test.json\",\"r\",encoding='utf-8') as f:\n",
    "    num = 0\n",
    "    for line in f.readlines():\n",
    "        sentence_lst = []  # 初始化句子列表\n",
    "        content = json.loads(line) # 将字符串数据转为字典\n",
    "        if 'A' in content.keys():\n",
    "            content_A = content['A'].split(\"\\n\\n\")[1].strip() # 获取语料\n",
    "            content_length = len(content_A)\n",
    "            # 向前切分句子\n",
    "            # 否则，则直接找到最大限度开始向前切分\n",
    "            for k in range(content_length-1,0,-1):\n",
    "                if content_A[k] in [\"。\",\"；\"]: \n",
    "                    sentence_lst.append(content_A[-1:k:-1][::-1])\n",
    "                    content_A = content_A[:k+1]\n",
    "            else:\n",
    "                if content_length > 512:\n",
    "                    content_A = content_A[:512]\n",
    "            \n",
    "            sentence_lst.append(content_A)  # 将最后的语料压入\n",
    "            #print(sentence_lst)\n",
    "            sentence_aug_lst = [data_augment(x) for x in sentence_lst]  # 进行数据增强\n",
    "            #print(sentence_aug_lst)\n",
    "            \n",
    "            for a,b in zip(sentence_aug_lst,sentence_lst):\n",
    "                #print(a) \n",
    "                if a == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    wrong_ids = []\n",
    "                    for k in range(len(a)):\n",
    "                        if a[k] != b[k]:\n",
    "                            wrong_ids.append(k)  # 获得错误文字所在位置\n",
    "                        \n",
    "                    idx = \"CAIL2019-SCM-A\"+str(num)\n",
    "                    raw_data.append({'id':idx, 'original_text':a, 'wrong_ids':wrong_ids, 'correct_text':b})  # 填入字典\n",
    "                    num += 1\n",
    "                \n",
    "            sentence_lst = []  # 清空句子列表\n",
    "        \n",
    "        if 'B' in content.keys():\n",
    "            content_B = content['B'].split(\"\\n\\n\")[1].strip() # 获取语料\n",
    "            content_length = len(content_B)\n",
    "            # 向前切分句子\n",
    "            # 否则，则直接找到最大限度开始向前切分\n",
    "            for k in range(content_length-1,0,-1):\n",
    "                if content_B[k] in [\"。\",\"；\"]: \n",
    "                    sentence_lst.append(content_B[-1:k:-1][::-1])\n",
    "                    content_B = content_B[:k+1]\n",
    "            else:\n",
    "                if content_length > 512:\n",
    "                    content_B = content_B[:512]\n",
    "            \n",
    "            sentence_lst.append(content_B)  # 将最后的语料压入\n",
    "        \n",
    "            sentence_aug_lst = [data_augment(x) for x in sentence_lst]  # 进行数据增强\n",
    "            \n",
    "            for a,b in zip(sentence_aug_lst,sentence_lst):\n",
    "                if a == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    wrong_ids = []\n",
    "                    for k in range(len(a)):\n",
    "                        if a[k] != b[k]:\n",
    "                            wrong_ids.append(k)  # 获得错误文字所在位置\n",
    "                        \n",
    "                    idx = \"CAIL2019-SCM-B\"+str(num)\n",
    "                    raw_data.append({'id':idx, 'original_text':a, 'wrong_ids':wrong_ids, 'correct_text':b})  # 填入字典\n",
    "                    num += 1\n",
    "                \n",
    "            sentence_lst = []  # 清空句子列表\n",
    "            \n",
    "        if 'C' in content.keys():\n",
    "            content_C = content['C'].split(\"\\n\\n\")[1].strip() # 获取语料\n",
    "            content_length = len(content_C)\n",
    "            # 向前切分句子\n",
    "            # 否则，则直接找到最大限度开始向前切分\n",
    "            for k in range(content_length-1,0,-1):\n",
    "                if content_C[k] in [\"。\",\"；\"]: \n",
    "                    sentence_lst.append(content_C[-1:k:-1][::-1])\n",
    "                    content_C = content_C[:k+1]\n",
    "            else:\n",
    "                if content_length > 512:\n",
    "                    content_C = content_C[:512]\n",
    "            \n",
    "            sentence_lst.append(content_C)  # 将最后的语料压入\n",
    "        \n",
    "            sentence_aug_lst = [data_augment(x) for x in sentence_lst]  # 进行数据增强\n",
    "            \n",
    "            for a,b in zip(sentence_aug_lst,sentence_lst):\n",
    "                if a == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    wrong_ids = []\n",
    "                    for k in range(len(a)):\n",
    "                        if a[k] != b[k]:\n",
    "                            wrong_ids.append(k)  # 获得错误文字所在位置\n",
    "                        \n",
    "                    idx = \"CAIL2019-SCM-C\"+str(num)\n",
    "                    raw_data.append({'id':idx, 'original_text':a, 'wrong_ids':wrong_ids, 'correct_text':b})  # 填入字典\n",
    "                    num += 1\n",
    "                \n",
    "            sentence_lst = []  # 清空句子列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "623e35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, json_path, mode='w', encoding='utf-8'):\n",
    "    dir = os.path.dirname(os.path.abspath(json_path))\n",
    "    if not os.path.exists(dir):\n",
    "        print(dir)\n",
    "        os.makedirs(dir)\n",
    "    with open(json_path, mode=mode, encoding=encoding) as f:\n",
    "        f.write(json.dumps(data, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b118bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/fengran/Desktop/文本挖掘技术/小组作业/law-mining/CAIL2019-SCM/output\n"
     ]
    }
   ],
   "source": [
    "save_json(raw_data,\"CAIL2019-SCM/output/test.json\") #将其存储起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7216ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ec49605",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CAIL2019-SCM/train.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        sentence_lst = []  # 初始化句子列表\n",
    "        content = json.loads(line) # 将字符串数据转为字典\n",
    "        if 'A' in content.keys():\n",
    "            content_A = content['A'].split(\"\\n\\n\")[1].strip() # 获取语料\n",
    "            content_length = len(content_A)\n",
    "            # 向前切分句子\n",
    "            # 否则，则直接找到最大限度开始向前切分\n",
    "            for k in range(content_length-1,0,-1):\n",
    "                if content_A[k] in [\"。\",\"；\"]: \n",
    "                    sentence_lst.append(content_A[-1:k:-1][::-1])\n",
    "                    content_A = content_A[:k+1]\n",
    "            else:\n",
    "                if content_length > 512:\n",
    "                    content_A = content_A[:512]\n",
    "            \n",
    "            sentence_lst.append(content_A)  # 将最后的语料压入\n",
    "            #print(sentence_lst)\n",
    "            sentence_aug_lst = [data_augment(x) for x in sentence_lst]  # 进行数据增强\n",
    "            #print(sentence_aug_lst)\n",
    "            \n",
    "            for a,b in zip(sentence_aug_lst,sentence_lst):\n",
    "                #print(a) \n",
    "                if a == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    wrong_ids = []\n",
    "                    for k in range(len(a)):\n",
    "                        if a[k] != b[k]:\n",
    "                            wrong_ids.append(k)  # 获得错误文字所在位置\n",
    "                        \n",
    "                    idx = \"CAIL2019-SCM-A\"+str(num)\n",
    "                    raw_data.append({'id':idx, 'original_text':a, 'wrong_ids':wrong_ids, 'correct_text':b})  # 填入字典\n",
    "                    num += 1\n",
    "                \n",
    "            sentence_lst = []  # 清空句子列表\n",
    "        \n",
    "        if 'B' in content.keys():\n",
    "            content_B = content['B'].split(\"\\n\\n\")[1].strip() # 获取语料\n",
    "            content_length = len(content_B)\n",
    "            # 向前切分句子\n",
    "            # 否则，则直接找到最大限度开始向前切分\n",
    "            for k in range(content_length-1,0,-1):\n",
    "                if content_B[k] in [\"。\",\"；\"]: \n",
    "                    sentence_lst.append(content_B[-1:k:-1][::-1])\n",
    "                    content_B = content_B[:k+1]\n",
    "            else:\n",
    "                if content_length > 512:\n",
    "                    content_B = content_B[:512]\n",
    "            \n",
    "            sentence_lst.append(content_B)  # 将最后的语料压入\n",
    "        \n",
    "            sentence_aug_lst = [data_augment(x) for x in sentence_lst]  # 进行数据增强\n",
    "            \n",
    "            for a,b in zip(sentence_aug_lst,sentence_lst):\n",
    "                if a == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    wrong_ids = []\n",
    "                    for k in range(len(a)):\n",
    "                        if a[k] != b[k]:\n",
    "                            wrong_ids.append(k)  # 获得错误文字所在位置\n",
    "                        \n",
    "                    idx = \"CAIL2019-SCM-B\"+str(num)\n",
    "                    raw_data.append({'id':idx, 'original_text':a, 'wrong_ids':wrong_ids, 'correct_text':b})  # 填入字典\n",
    "                    num += 1\n",
    "                \n",
    "            sentence_lst = []  # 清空句子列表\n",
    "            \n",
    "        if 'C' in content.keys():\n",
    "            content_C = content['C'].split(\"\\n\\n\")[1].strip() # 获取语料\n",
    "            content_length = len(content_C)\n",
    "            # 向前切分句子\n",
    "            # 否则，则直接找到最大限度开始向前切分\n",
    "            for k in range(content_length-1,0,-1):\n",
    "                if content_C[k] in [\"。\",\"；\"]: \n",
    "                    sentence_lst.append(content_C[-1:k:-1][::-1])\n",
    "                    content_C = content_C[:k+1]\n",
    "            else:\n",
    "                if content_length > 512:\n",
    "                    content_C = content_C[:512]\n",
    "            \n",
    "            sentence_lst.append(content_C)  # 将最后的语料压入\n",
    "        \n",
    "            sentence_aug_lst = [data_augment(x) for x in sentence_lst]  # 进行数据增强\n",
    "            \n",
    "            for a,b in zip(sentence_aug_lst,sentence_lst):\n",
    "                if a == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    wrong_ids = []\n",
    "                    for k in range(len(a)):\n",
    "                        if a[k] != b[k]:\n",
    "                            wrong_ids.append(k)  # 获得错误文字所在位置\n",
    "                        \n",
    "                    idx = \"CAIL2019-SCM-C\"+str(num)\n",
    "                    raw_data.append({'id':idx, 'original_text':a, 'wrong_ids':wrong_ids, 'correct_text':b})  # 填入字典\n",
    "                    num += 1\n",
    "                \n",
    "            sentence_lst = []  # 清空句子列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e85895",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(raw_data,\"CAIL2019-SCM/output/train.json\") #将其存储起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a805da85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CAIL2019-SCM/valid.json\",\"r\",encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        sentence_lst = []  # 初始化句子列表\n",
    "        content = json.loads(line) # 将字符串数据转为字典\n",
    "        if 'A' in content.keys():\n",
    "            content_A = content['A'].split(\"\\n\\n\")[1].strip() # 获取语料\n",
    "            content_length = len(content_A)\n",
    "            # 向前切分句子\n",
    "            # 否则，则直接找到最大限度开始向前切分\n",
    "            for k in range(content_length-1,0,-1):\n",
    "                if content_A[k] in [\"。\",\"；\"]: \n",
    "                    sentence_lst.append(content_A[-1:k:-1][::-1])\n",
    "                    content_A = content_A[:k+1]\n",
    "            else:\n",
    "                if content_length > 512:\n",
    "                    content_A = content_A[:512]\n",
    "            \n",
    "            sentence_lst.append(content_A)  # 将最后的语料压入\n",
    "            #print(sentence_lst)\n",
    "            sentence_aug_lst = [data_augment(x) for x in sentence_lst]  # 进行数据增强\n",
    "            #print(sentence_aug_lst)\n",
    "            \n",
    "            for a,b in zip(sentence_aug_lst,sentence_lst):\n",
    "                #print(a) \n",
    "                if a == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    wrong_ids = []\n",
    "                    for k in range(len(a)):\n",
    "                        if a[k] != b[k]:\n",
    "                            wrong_ids.append(k)  # 获得错误文字所在位置\n",
    "                        \n",
    "                    idx = \"CAIL2019-SCM-A\"+str(num)\n",
    "                    raw_data.append({'id':idx, 'original_text':a, 'wrong_ids':wrong_ids, 'correct_text':b})  # 填入字典\n",
    "                    num += 1\n",
    "                \n",
    "            sentence_lst = []  # 清空句子列表\n",
    "        \n",
    "        if 'B' in content.keys():\n",
    "            content_B = content['B'].split(\"\\n\\n\")[1].strip() # 获取语料\n",
    "            content_length = len(content_B)\n",
    "            # 向前切分句子\n",
    "            # 否则，则直接找到最大限度开始向前切分\n",
    "            for k in range(content_length-1,0,-1):\n",
    "                if content_B[k] in [\"。\",\"；\"]: \n",
    "                    sentence_lst.append(content_B[-1:k:-1][::-1])\n",
    "                    content_B = content_B[:k+1]\n",
    "            else:\n",
    "                if content_length > 512:\n",
    "                    content_B = content_B[:512]\n",
    "            \n",
    "            sentence_lst.append(content_B)  # 将最后的语料压入\n",
    "        \n",
    "            sentence_aug_lst = [data_augment(x) for x in sentence_lst]  # 进行数据增强\n",
    "            \n",
    "            for a,b in zip(sentence_aug_lst,sentence_lst):\n",
    "                if a == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    wrong_ids = []\n",
    "                    for k in range(len(a)):\n",
    "                        if a[k] != b[k]:\n",
    "                            wrong_ids.append(k)  # 获得错误文字所在位置\n",
    "                        \n",
    "                    idx = \"CAIL2019-SCM-B\"+str(num)\n",
    "                    raw_data.append({'id':idx, 'original_text':a, 'wrong_ids':wrong_ids, 'correct_text':b})  # 填入字典\n",
    "                    num += 1\n",
    "                \n",
    "            sentence_lst = []  # 清空句子列表\n",
    "            \n",
    "        if 'C' in content.keys():\n",
    "            content_C = content['C'].split(\"\\n\\n\")[1].strip() # 获取语料\n",
    "            content_length = len(content_C)\n",
    "            # 向前切分句子\n",
    "            # 否则，则直接找到最大限度开始向前切分\n",
    "            for k in range(content_length-1,0,-1):\n",
    "                if content_C[k] in [\"。\",\"；\"]: \n",
    "                    sentence_lst.append(content_C[-1:k:-1][::-1])\n",
    "                    content_C = content_C[:k+1]\n",
    "            else:\n",
    "                if content_length > 512:\n",
    "                    content_C = content_C[:512]\n",
    "            \n",
    "            sentence_lst.append(content_C)  # 将最后的语料压入\n",
    "        \n",
    "            sentence_aug_lst = [data_augment(x) for x in sentence_lst]  # 进行数据增强\n",
    "            \n",
    "            for a,b in zip(sentence_aug_lst,sentence_lst):\n",
    "                if a == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    wrong_ids = []\n",
    "                    for k in range(len(a)):\n",
    "                        if a[k] != b[k]:\n",
    "                            wrong_ids.append(k)  # 获得错误文字所在位置\n",
    "                        \n",
    "                    idx = \"CAIL2019-SCM-C\"+str(num)\n",
    "                    raw_data.append({'id':idx, 'original_text':a, 'wrong_ids':wrong_ids, 'correct_text':b})  # 填入字典\n",
    "                    num += 1\n",
    "                \n",
    "            sentence_lst = []  # 清空句子列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29a5390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(raw_data,\"CAIL2019-SCM/output/dev.json\") #将其存储起来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75386a83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
